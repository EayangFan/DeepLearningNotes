第九章 卷积神经网络
卷积网络（convolutional network）（LeCun，1989），也叫卷积神经网络（convolutional neural network, CNN），是一种专门用来处理具有类似网络结构的数据的神经网络。例如时间序列数据（可以认为是在时间轴上有规律地采样形成的一维网络）和图像数据（可以看作二维的像素网格）。卷积网络在诸多应用领域都表现优异。“卷积神经网络”一词表名该网络使用了卷积（convolution）这种数学运算。卷积是一种特殊的线性运算。卷积网络是指那些至少在网络的一层中使用卷及运算来替代一般的矩阵乘法运算的神经网络。

P201 9.1 卷积运算
（9.1）连续形式，（9.2）简便表示形式，（9.3）一维离散形式，（9.4）二维离散形式，（9.6）互相关函数（cross-correlation），许多机器学习的库实现的是互相关函数但是称之为卷积。在机器学习中，学习算法会在核合适的位置学得恰当的值。
离散卷积可以看作矩阵的乘法

P203 9.2 动机
稀疏交互（sparse interactions）
参数共享（parameter sharing）
等变表示（equivariant representations）

P207 9.3 池化
卷积网络中一个典型层包括三级。在第一级中，这一层并行地计算多个卷积产生一组线性激活相应。在第二级中，每一个线性激活相应将会通过一个非线性的激活函数，例如整流线性激活函数。这一级有时也被称为探测级（detector stage）。在第三级中，我们使用池化函数（pooling function）来进一步调整这一层的输出。
池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出，例如最大池化函数（max pooling）函数（Zhou and Chellappa，1988）
池化能够帮助输入的表示近似不变（invariant）。局部平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现而不关心他出现的具体位置时。
使用池化可以看做增加了一个无限强的先验：这一层学得的函数必须具有对少量平移的不变性。当这个假设成立时，池化可以极大地提高网络的统计效率。

P210 9.4 卷积与池化作为一种无限强的先验
先验概率分布（prior probability distribution）是一个模型参数的概率分布。他刻画了我们在看到数据之前认为什么样的模型是合理的信念。
一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值，无论数据对于这些参数的值给出了多大的支持。
我们可以把卷积网络类比成一个全连接网络，但对于这个全连接网络的权重有一个无限强的先验。这个无限强的先验是说一个隐藏单元的权重必须和他邻居的权重相同，但可以在空间上移动。
总之，我们可以把卷积的使用当做对网络中一层的参数引入了一个无限强的先验概率分布。这个先验说明了该层应该学到的函数只包含拒不连接关系并且对平移具有等变性。类似的，使用池化也是一个无限强的先验：每一个单元都具有对少量平移的不变性。
卷积和池化可能导致欠拟合。与任何其他先验类似，卷积和池化只有当先验的假设合理且正确时才会有用。如果一项任务依赖于保存精确的空间信息，那么在所有的特征上使用池化将增大训练误差。一些卷积网络结构（Szegedy et al.,2014a）为了既获得具有较高不变性的特征又获得当平移不变性不合理时不会导致欠拟合的特征，被设计成在一些通道上使用池化而另一些通道上不使用。

P211 9.5 基本卷积函数的变体
神经网络提到的卷积和数学文献中的标准离散卷及运算并不相同。
首先，神经网络中的卷积通常是指由多个并行卷积组成的运算。这是因为具有单个核的卷积只能提取一种类型的特征。我们通常希望网络的每一层能提取多种类型的特征。
输入通常也不仅仅是实值的网络，而是由一系列观测数据的向量构成的网络。比如彩色图像有三个通道。
下采样（downsampling）、带有步幅的卷积的使用可以用来减小计算负担（图9.12）
局部连接的网络层也叫非共享卷积（unshared convolution）（图9.14比较了局部连接，卷积和全连接）
平铺卷积（tiled convolution）（Gregor and LeCun, 2010a; Le et al., 2010）（图9.16比较了局部连接层，平铺卷积和标准卷积）

P218 9.6 结构化输出
卷积神经网络可以用于输出高维的结构化对象，而不仅仅是预测分类任务的类标签或回归任务的实数值。通常这个对象只是一个张量，由标准卷积层产生。例如，模型可以产生张量S，其中Si,j,k是网络的输入像素（j, k）属于类i的概率。这允许模型标记图像中的每个像素，并绘制沿着单个对象轮廓的精确掩模。
这可以用来做图像在区域上的分割。

P219 9.7 数据类型
卷积网络使用的数据通常包含多个通道，每个通道是时间或空间中某一点的不同观测量。参考表9.1体会不同的数据类型的例子。
疑惑：对于图像具有不同的高度和宽度的数据集，卷积核可以使用不同次，卷积运算的输出就会相应的放缩（这样模型不就随之改变了吗...）

P220 9.9 随机或无监督的特征
通常，卷积网络训练中最昂贵的部分是学习特征。输出层的计算代价通常不高，因为在通过若干层池化之后作为该层输入的特征的数量较少。当使用梯度下降执行监督训练时，每步梯度计算都需要完整地运行整个网络的前向传播和反向传播。减少卷积网络训练成本的一种方式是使用那些不是由监督方式训练得到的特征。
有三种基本策略可以不通过监督训练而得到卷积核。其中一种是简单地随机初始化他们，另一种是手动设计他们，最后一种是使用无监督的方法来学习特征（第三部分会提到，还没看）。

P221 9.10 卷积网络的神经科学基础
卷积网络也许是生物学启发人工智能的最为成功的案例。
卷积网络的历史始于神经科学实验，David Hubel和Torsten Wiesel合作多年，他们的成就最终获得了诺贝尔奖。他们发现对于猫来说，处于视觉系统较为前面的神经元对非常特定的光模式（如精确定向的条纹）反应最为强烈，但对其他模式几乎完全没有反应。
（神经元被特定的模式激活）

P226 9.11 卷积网络与深度学习的历史
20世纪90年代ATT的神经网络研究小组就开发了一个用于读取支票的卷积神经网络。90年代末这个系统已经被用于读取美国10%以上的支票。后来微软部署了若干基于卷积神经网络的手写识别系统（Simard et al,. 2003）
当前对深度学习的商业兴趣的热度始于Krizhevsky et al.(2012a)赢得了ImageNet对象识别挑战。
AlexNet共有八层网络，其结构如下：
卷积层1：输入224*224*3    卷积核11*11*3*96 步长为4  然后是ReLU 、局部归一化 、3*3步长为2的最大值池化
卷积层2：输入28*28*96    卷积核5*5*96*256  然后是ReLU、局部归一化、3*3步长为2的最大值池化
卷积层3：输入14*14*256    卷积核3*3*256*384  然后是ReLU
卷积层4：输入14*14*384    卷积 核3*3*384*384  然后是ReLU
卷积层5：输入14*14*384    卷积核3*3*384*256  然后是ReLU、3*3步长为2的最大值池化
全连接层1：输入7*7*256    输出4096  然后是ReLU、DropOut
全连接层2：输入4096    输出4096  然后是ReLU、DropOut
全连接层3：输入4096    输出1000
