
第六章深度前馈网络
P105 前言
深度前馈网络（deep feedforward network），也叫作前馈神经网络（feedforward neural network）或者多层感知机（multilayer perceptron, MLP）是典型的深度学习模型。前馈网络的目标是近似某个函数f*。例如，对于分类器y=f*(x)将输入x映射到一个类别y。前馈网络定义了一个映射y=f(x,; theta)并且学习参数theta的值，使它能够得到最佳的函数近似。
这种模型被称为前向（feedforward）的，是因为信息流过x的函数，流经用于定义f的中间计算过程，最终到达输出y。在模型的输出和模型本身之间没有反馈（feedback）连接。当前馈神经网络被扩展成包含反馈连接时，他们被称为循环神经网络（recurrent neural network）

链的全长被称为模型的深度（depth），前馈网络的最后一层被称为输出层（output layer）。中间的层被称为隐藏层（hidden layer）。也可以把层想象由许多并行操作的单元（unit）组成，每个单元表示一个向量到标量的函数。隐藏层有激活函数（activation function），通常优化过程会用到反向传播（back propagation）算法。
我们最好将前馈神经网络想成是为了实现统计泛化而设计出的函数近似机，它偶尔从我们了解的大脑中提取灵感，但并不是大脑功能的模型。


P111 6.2.1代价函数
在大多数情况下，参数模型定义了一个分布p（y| x ; theta）并且简单地使用最大似然原理。这意味着我们使用训练数据和模型预测间的交叉熵作为代价函数。
使用最大似然学习条件分布

P113 6.2.2 输出单元
用于高斯输出分布的线性单元，用于Bernoulli输出分布的sigmoid单元，用于Multinoulli输出分布的softmax单元，其他的输出模型（异方差性）

P119 6.3隐藏单元
隐藏单元用在模型的隐藏层中，它的设计是一个非常活跃的研究领域，并且还没有许多明确的指导性理论原则。通常是先直觉认为某种隐藏单元可能表现良好，然后用它组成神经网络进行训练，最后验证集来评估它的性能。并不是所有的隐藏单元都处处可微，但在实践中，梯度下降对这些机器学习模型仍然表现得足够好。部分原因是神经网络训练算法通常不会达到代价函数的局部极小值，而是仅仅显著的减小它的值。大多数的隐藏单元都可以描述为接受输入向量x，进行放射变换z=Wt x + b，然后使用一个逐元素的非线性函数g(z)。大多数隐藏单元的区别仅仅在于激活函数g(z)的形式。

整流线性单元（recitified linear unit）是隐藏单元极好的默认选择。扩展出了绝对值整流（absolute value rectification），渗漏整流线性单元（Leaky ReLU）和参数化整流线性单元（parametric ReLU）
Maxout单元（maxout unit）进一步扩展了整流线性单元。它将z划分为每组具有k个值的组，而不是使用作用于每个元素的函数g(z)。每个maxout单元则输出每组中的最大元素。
Logistic sigmoid与双曲正切函数

P123 6.4架构设计
架构（architecture）一词是指网络的整体结构：他应该具有多少单元，以及这些单元应该如何连接。在链式架构中，主要的架构考虑是选择网络的深度和每一层的宽度。
万能近似定理（universal approximation theorem）（Hornik et al., 1990; Cybenko, 1989）表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种“挤压”性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给与网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个由县委空间到另一个有限维空间的Borel可测函数。前馈网络的导数也可以任意好的近似函数的导数（Hornik et al., 1990）
万能近似定理意味着无论我们试图学习什么函数，我们知道一个大的MLP一定能够表示这个函数。然而我们不能保证训练算法能够学得这个函数。及时MLP能够表示该函数，学习也可能因两个不同的原因而失败。首先，用于训练的优化算法可能找不到用于期望函数的参数值。其次，训练算法可能由于过拟合而选择了错误的函数。总之，具有单层的前馈网络足以表示任何函数，但是网络层可能大的不可实现，并且可能无法正确的学习和泛化。在很多情况下，使用更深的模型能够减少表示期望函数所需的单元的数量，并且可以减少泛化误差。

P126 6.5 反向传播
信息通过网络向前流动，输入x提供初始信息，然后传播到每一层的隐藏单元，最终产生输出y，这称之为前向传播（forward propagation）。在训练过程中，前向传播可以持续向前知道它产生一个标量代价函数J（theta）。反向传播（back propagation）算法（Rumelhart et al., 1986c）经常简称为backprop，允许来自代价函数的信息通过网络向后流动，以便计算梯度。
事实上，反向传播仅指用于计算梯度的方法，而另一种算法，例如随机梯度下降，使用该梯度来进行学习https://www.bilibili.com/video/av53925783（b站不错的视频，可以1.5倍速放一下）

P139 6.6 历史小记
处于反向传播算法底层的链式法则是17世纪发明的（Leibniz, 1676; L’Hopital, 1696）。19世纪Cauchy（1847）引入梯度下降作为迭代问题的近似解法。
从20世纪40年代开始，这些函数近似技术被用于导出感知机的机器学习模型。然而，最早的模型都是基于线性的模型。来自包括Marvin Minsky的批评指出来线性模型族的几个缺陷，例如它无法学习亦或函数，这导致了对整个神经网络方法的抵制。
学习非线性函数需要多层感知机的发展和计算该模型梯度的方法。基于动态规划的连式法则的高效应用开始出现在20世纪60和70年代，Werbos（1981）提出用这些技术来训练人工神经网络。这个想法以不同的方式被重发现后（LeCun, 1985; Parker, 1985; Rumelhart et al., 1986a），最终在实践中得以发展。
在反向传播成功后，神经网络研究获得了普及，并在20世纪90年代初达到高峰。随后，其他机器学习技术变的更受欢迎，直到2006年开始的现代深度学习复兴。
现代前馈网络的核心思想自20世纪80年代以来没有发生重大变化，仍然使用相同的反向传播算法和相同的梯度下降方法。1986年-2015年，神经网络性能的大部分改进可归因于两个因素：第一，较大的数据集减少了统计繁华对神经网络的挑战的程度。第二，神经网络由于更强大的计算机和更好的软件基础设施已经变得更大。
少量的算法变化也显著改善了神经网络的性能。其一是用交叉熵族损失函数替代军方误差损失函数。交叉熵逐渐替代了军方误差，并且最大似然原理的想法在统计学界和机器学习界之间广泛传播。使用交叉熵损失大大提高了具有sigmoid和softmax输出的模型的性能，而当使用军方误差损失时会存在饱和和学习缓慢的问题。
另一个显著改善前馈神经网络性能的算法上的主要变化是使用分段线性隐藏单元来替代sigmoid隐藏单元，例如用整流线性单元。整流线性单元在早期很普及，但在20世纪80年代很大程度上被sigmoid取代，到21世纪初甚至有观点认为，必须避免具有不可导点的激活函数，所以避免了整流线性单元。直到2009年开始发生改变。Jarrett et al.（2009b）观察到，在神经网络结构设计的几个不同因素中“使用整流非线性是提高识别系统性能的最重要的唯一因素”
整流线性单元还具有历史意义，因为他们表明神经科学继续对深度学习算法的发展产生影响。
当2006年深度学习开始现代复兴时，前馈网络仍然有不良的声誉。从2006-2012年，人们普遍认为，前馈网络不会表现良好，除非他们得到其他模型的辅助，例如概率模型。现在已经知道（2016），只要具备适当的资源和工程实践，前馈网络表现得非常好。
